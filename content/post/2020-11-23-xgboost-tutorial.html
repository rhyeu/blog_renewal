---
title: xgboost tutorial
author: 류성균
date: '2020-11-23'
slug: index.en-us
categories:
  - Today I learn
tags:
  - xgboost
  - kaggle
  - machine-learning
keywords:
  - tech
editor_options: 
  chunk_output_type: console
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<!--more-->
<pre class="r"><code># install.packages(&quot;xgboost&quot;)
# install.packages(&quot;DiagrammeR&quot;) # for tree visulization

library(xgboost)
library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages ---------------------- tidyverse 1.3.0 --</code></pre>
<pre><code>## √ ggplot2 3.3.2     √ purrr   0.3.4
## √ tibble  3.0.3     √ dplyr   1.0.2
## √ tidyr   1.1.2     √ stringr 1.4.0
## √ readr   1.3.1     √ forcats 0.5.0</code></pre>
<pre><code>## -- Conflicts ------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
## x dplyr::slice()  masks xgboost::slice()</code></pre>
<div id="예제-데이터셋-식용-버섯-구별하기-in-uci-machine-learning-repository" class="section level3">
<h3>예제 데이터셋 : 식용 버섯 구별하기 in UCI machine learning repository</h3>
<pre class="r"><code>data(agaricus.train, package = &#39;xgboost&#39;)
data(agaricus.test, package = &#39;xgboost&#39;)

train &lt;- agaricus.train
test &lt;- agaricus.test</code></pre>
<pre class="r"><code>glimpse(train) # label : target variable</code></pre>
<pre><code>## List of 2
##  $ data :Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   .. ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
##   .. ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...
##   .. ..@ Dim     : int [1:2] 6513 126
##   .. ..@ Dimnames:List of 2
##   .. ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..@ factors : list()
##  $ label: num [1:6513] 1 0 0 1 0 0 0 1 0 0 ...</code></pre>
<pre class="r"><code>dim(train$data)</code></pre>
<pre><code>## [1] 6513  126</code></pre>
<pre class="r"><code>dim(test$data)</code></pre>
<pre><code>## [1] 1611  126</code></pre>
<pre class="r"><code>class(train$data)[1] # sparse matrix</code></pre>
<pre><code>## [1] &quot;dgCMatrix&quot;</code></pre>
<pre class="r"><code>class(train$label)[1]</code></pre>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
</div>
<div id="basic-training-using-xgboost" class="section level3">
<h3>Basic training using XGBoost</h3>
<div id="주요한-parameter" class="section level4">
<h4>주요한 parameter</h4>
<ul>
<li>objective = “binary:logistric”</li>
<li>max.depth = 2 : the trees won’t be deep</li>
<li>nthread = 2 : cpu 갯수</li>
<li>nrounds = 2 : there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.</li>
</ul>
<pre class="r"><code>bstSparse &lt;- xgboost(data = train$data,
                     label = train$label, 
                     max.depth = 2,
                     eta = 1, 
                     nthread = 2,
                     nrounds = 2,
                     objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-error:0.046522 
## [2]  train-error:0.022263</code></pre>
</div>
<div id="parameter-variations" class="section level4">
<h4>Parameter variations</h4>
<div id="dense-matrix" class="section level5">
<h5>Dense matrix</h5>
<pre class="r"><code>bstDense &lt;- xgboost(data = as.matrix(train$data),
                     label = train$label, 
                     max.depth = 2,
                     eta = 1, 
                     nthread = 2,
                     nrounds = 2,
                     objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-error:0.046522 
## [2]  train-error:0.022263</code></pre>
</div>
<div id="xgb.dmatrix-data-label-같이" class="section level5">
<h5>xgb.DMatrix : data, label 같이</h5>
<pre class="r"><code>dtrain &lt;- xgb.DMatrix(data = train$data, label = train$label)
bstDMatrix &lt;- xgboost(data=dtrain, 
                      max.depth = 2,
                      eta = 1, 
                      nthread = 2,
                      nrounds = 2,
                      objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-error:0.046522 
## [2]  train-error:0.022263</code></pre>
</div>
<div id="verbose-option" class="section level5">
<h5>verbose option</h5>
<pre class="r"><code># verbose = 0, no message
bst &lt;- xgboost(data=dtrain, 
                      max.depth = 2,
                      eta = 1, 
                      nthread = 2,
                      nrounds = 2,
                      objective = &quot;binary:logistic&quot;,
                      verbose = 0)</code></pre>
<pre class="r"><code># verbose = 1, print evaluation metric

bst &lt;- xgboost(data=dtrain, 
                      max.depth = 2,
                      eta = 1, 
                      nthread = 2,
                      nrounds = 2,
                      objective = &quot;binary:logistic&quot;,
                      verbose = 1)</code></pre>
<pre><code>## [1]  train-error:0.046522 
## [2]  train-error:0.022263</code></pre>
<pre class="r"><code># verbose = 2, also print information about tree

bst &lt;- xgboost(data=dtrain, 
                      max.depth = 2,
                      eta = 1, 
                      nthread = 2,
                      nrounds = 2,
                      objective = &quot;binary:logistic&quot;,
                      verbose = 1)</code></pre>
<pre><code>## [1]  train-error:0.046522 
## [2]  train-error:0.022263</code></pre>
</div>
</div>
</div>
<div id="basic-pridiction-using-xgboost" class="section level3">
<h3>Basic Pridiction using XGBoost</h3>
<pre class="r"><code>pred &lt;- predict(bst, test$data)

# size of the prediction vector
print(length(pred))</code></pre>
<pre><code>## [1] 1611</code></pre>
<pre class="r"><code>## 확률이 출력됨
print(head(pred))</code></pre>
<pre><code>## [1] 0.28583017 0.92392391 0.28583017 0.28583017 0.05169873 0.92392391</code></pre>
<pre class="r"><code>prediction &lt;- as.numeric(pred &gt; 0.5)
print(head(prediction))</code></pre>
<pre><code>## [1] 0 1 0 0 0 1</code></pre>
<div id="measuring-model-performance" class="section level4">
<h4>Measuring model performance</h4>
<pre class="r"><code>err &lt;- mean(as.numeric(pred &gt; 0.5) != test$label)
print(paste(&quot;test-error=&quot;, err)) # &quot;test-error= 0.0217256362507759&quot;</code></pre>
<pre><code>## [1] &quot;test-error= 0.0217256362507759&quot;</code></pre>
</div>
</div>
<div id="advanced-features" class="section level3">
<h3>Advanced features</h3>
<div id="dataset-preparation" class="section level4">
<h4>Dataset preparation</h4>
<ul>
<li>xgb.DMatrix 객체를 사용하면 매 라운드마다 report를 해줌</li>
</ul>
<pre class="r"><code>dtrain &lt;- xgb.DMatrix(data = train$data, label = train$label)
dtest &lt;- xgb.DMatrix(data = test$data, label = test$label)</code></pre>
<pre class="r"><code>watchlist &lt;- list(train = dtrain, test = dtest)
bst &lt;- xgb.train(data = dtrain, 
                 max.depth = 2, 
                 eta = 1, 
                 nthread = 2, 
                 nrounds = 2, 
                 watchlist = watchlist,
                 objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-error:0.046522    test-error:0.042831 
## [2]  train-error:0.022263    test-error:0.021726</code></pre>
<ul>
<li>다른 metric을 사용해보자</li>
</ul>
<pre class="r"><code>bst &lt;- xgb.train(data = dtrain, 
                 max.depth = 2,
                 eta = 1,
                 nthread = 2,
                 nrounds = 2,
                 watchlist = watchlist,
                 eval.metric = &quot;error&quot;,
                 eval.metric = &quot;logloss&quot;,
                 objective = &quot;binary:logistic&quot;
                 )</code></pre>
<pre><code>## [1]  train-error:0.046522    train-logloss:0.233366  test-error:0.042831 test-logloss:0.226687 
## [2]  train-error:0.022263    train-logloss:0.136656  test-error:0.021726 test-logloss:0.137875</code></pre>
<ul>
<li>decision tree가 아닌 linear boosting을 해보자</li>
</ul>
<pre class="r"><code>bst &lt;- xgb.train(data = dtrain, 
                 booster = &quot;gblinear&quot;,
                 nthread = 2,
                 nrounds = 2,
                 watchlist = watchlist,
                 eval.metric = &quot;error&quot;,
                 eval.metric = &quot;logloss&quot;,
                 objective = &quot;binary:logistic&quot;
                 )</code></pre>
<pre><code>## [1]  train-error:0.018118    train-logloss:0.195446  test-error:0.018622 test-logloss:0.200063 
## [2]  train-error:0.004453    train-logloss:0.085744  test-error:0.004966 test-logloss:0.088158</code></pre>
</div>
</div>
<div id="manipulating-xgb.dmatrix" class="section level3">
<h3>Manipulating xgb.DMatrix</h3>
<div id="model-저장-loading" class="section level4">
<h4>model 저장 / loading</h4>
<pre class="r"><code>xgb.DMatrix.save(dtrain, &quot;dtrain.buffer&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>dtrain2 &lt;- xgb.DMatrix(&quot;dtrain.buffer&quot;)</code></pre>
<pre><code>## [15:33:57] 6513x126 matrix with 143286 entries loaded from dtrain.buffer</code></pre>
<pre class="r"><code>bst &lt;- xgb.train(data = dtrain2, 
                 max.depth = 2, 
                 eta = 1, 
                 nthread = 2, 
                 nrounds = 2,
                 watchlist = watchlist,
                 objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-error:0.046522    test-error:0.042831 
## [2]  train-error:0.022263    test-error:0.021726</code></pre>
</div>
<div id="information-extraction" class="section level4">
<h4>Information extraction</h4>
<pre class="r"><code>label &lt;- getinfo(dtest, &quot;label&quot;)
pred &lt;- predict(bst, dtest)
err &lt;- as.numeric(sum(as.integer(pred &gt; 0.5 ) != label)) / length(label)
print(paste(&quot;test-error=&quot;, err))</code></pre>
<pre><code>## [1] &quot;test-error= 0.0217256362507759&quot;</code></pre>
</div>
</div>
<div id="view-feature-importance-influence-from-the-learnt-model" class="section level3">
<h3>View feature importance / influence from the learnt model</h3>
<pre class="r"><code>importance_matrix &lt;- xgb.importance(model = bst)

print(importance_matrix)</code></pre>
<pre><code>##    Feature       Gain     Cover Frequency
## 1:      28 0.67615471 0.4978746       0.4
## 2:      55 0.17135375 0.1920543       0.2
## 3:      59 0.12317236 0.1638750       0.2
## 4:     108 0.02931918 0.1461960       0.2</code></pre>
<pre class="r"><code>xgb.plot.importance(importance_matrix = importance_matrix)</code></pre>
<p><img src="/post/2020-11-23-xgboost-tutorial_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div id="view-the-trees-from-a-model" class="section level4">
<h4>view the trees from a model</h4>
<pre class="r"><code>xgb.dump(bst, with_stats = T) # </code></pre>
<pre><code>##  [1] &quot;booster[0]&quot;                                                                   
##  [2] &quot;0:[f28&lt;-9.53674316e-07] yes=1,no=2,missing=1,gain=4000.53101,cover=1628.25&quot;   
##  [3] &quot;1:[f55&lt;-9.53674316e-07] yes=3,no=4,missing=3,gain=1158.21204,cover=924.5&quot;     
##  [4] &quot;3:leaf=1.71217716,cover=812&quot;                                                  
##  [5] &quot;4:leaf=-1.70044053,cover=112.5&quot;                                               
##  [6] &quot;2:[f108&lt;-9.53674316e-07] yes=5,no=6,missing=5,gain=198.173828,cover=703.75&quot;   
##  [7] &quot;5:leaf=-1.94070864,cover=690.5&quot;                                               
##  [8] &quot;6:leaf=1.85964918,cover=13.25&quot;                                                
##  [9] &quot;booster[1]&quot;                                                                   
## [10] &quot;0:[f59&lt;-9.53674316e-07] yes=1,no=2,missing=1,gain=832.545044,cover=788.852051&quot;
## [11] &quot;1:[f28&lt;-9.53674316e-07] yes=3,no=4,missing=3,gain=569.725098,cover=768.389709&quot;
## [12] &quot;3:leaf=0.78471756,cover=458.936859&quot;                                           
## [13] &quot;4:leaf=-0.968530357,cover=309.45282&quot;                                          
## [14] &quot;2:leaf=-6.23624468,cover=20.462389&quot;</code></pre>
<pre class="r"><code>xgb.plot.tree(model = bst)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       rankdir = \"LR\"]\n\nnode [color = \"DimGray\",\n      style = \"filled\",\n      fontname = \"Helvetica\"]\n\nedge [color = \"DimGray\",\n     arrowsize = \"1.5\",\n     arrowhead = \"vee\",\n     fontname = \"Helvetica\"]\n\n  \"1\" [label = \"Tree 1\n59\nCover: 788.852051\nGain: 832.545044\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"2\" [label = \"28\nCover: 768.389709\nGain: 569.725098\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"3\" [label = \"Leaf\nCover: 20.462389\nValue: -6.23624468\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"4\" [label = \"Leaf\nCover: 458.936859\nValue: 0.78471756\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"5\" [label = \"Leaf\nCover: 309.45282\nValue: -0.968530357\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"6\" [label = \"Tree 0\n28\nCover: 1628.25\nGain: 4000.53101\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"7\" [label = \"55\nCover: 924.5\nGain: 1158.21204\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"8\" [label = \"108\nCover: 703.75\nGain: 198.173828\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"9\" [label = \"Leaf\nCover: 812\nValue: 1.71217716\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"10\" [label = \"Leaf\nCover: 112.5\nValue: -1.70044053\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"11\" [label = \"Leaf\nCover: 690.5\nValue: -1.94070864\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"12\" [label = \"Leaf\nCover: 13.25\nValue: 1.85964918\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n\"1\"->\"2\" [label = \"< -9.53674316e-07\", style = \"bold\"] \n\"2\"->\"4\" [label = \"< -9.53674316e-07\", style = \"bold\"] \n\"6\"->\"7\" [label = \"< -9.53674316e-07\", style = \"bold\"] \n\"7\"->\"9\" [label = \"< -9.53674316e-07\", style = \"bold\"] \n\"8\"->\"11\" [label = \"< -9.53674316e-07\", style = \"bold\"] \n\"1\"->\"3\" [style = \"bold\", style = \"solid\"] \n\"2\"->\"5\" [style = \"solid\", style = \"solid\"] \n\"6\"->\"8\" [style = \"solid\", style = \"solid\"] \n\"7\"->\"10\" [style = \"solid\", style = \"solid\"] \n\"8\"->\"12\" [style = \"solid\", style = \"solid\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="save-and-load-models" class="section level3">
<h3>Save and load models</h3>
<pre class="r"><code>### save model to binary local file
xgb.save(bst, &quot;xgboost.model&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>bst2 &lt;- xgb.load(&quot;xgboost.model&quot;)
pred2 &lt;- predict(bst2, test$data)

# And now the test
print(paste(&quot;sum(abs(pred2-pred))=&quot;, sum(abs(pred2-pred))))</code></pre>
<pre><code>## [1] &quot;sum(abs(pred2-pred))= 0&quot;</code></pre>
<pre class="r"><code># save model to R&#39;s raw vector
rawVec &lt;- xgb.save.raw(bst)</code></pre>
<pre class="r"><code># print class
print(class(rawVec))</code></pre>
<pre><code>## [1] &quot;raw&quot;</code></pre>
<pre class="r"><code># load binary model to R
bst3 &lt;- xgb.load(rawVec)</code></pre>
<pre><code>## Warning in value[[3L]](cond): The model had been generated by XGBoost version
## 1.0.0 or earlier and was loaded from a RDS file. We strongly ADVISE AGAINST
## using saveRDS() function, to ensure that your model can be read in current
## and upcoming XGBoost releases. Please use xgb.save() instead to preserve
## models for the long term. For more details and explanation, see https://
## xgboost.readthedocs.io/en/latest/tutorials/saving_model.html</code></pre>
<pre class="r"><code>pred3 &lt;- predict(bst3, test$data)

# pred3 should be identical to pred
print(paste(&quot;sum(abs(pred3-pred))=&quot;, sum(abs(pred3-pred))))</code></pre>
<pre><code>## [1] &quot;sum(abs(pred3-pred))= 0&quot;</code></pre>
<ul>
<li>reference : <a href="https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html" class="uri">https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html</a></li>
</ul>
</div>
